{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# Developer: Ali Hashaam (ali.hashaam@initos.com) <br>\n",
    "\\# 10th January 2019 <br>\n",
    "\n",
    "\\# Â© 2019 initOS GmbH <br>\n",
    "\\# License MIT <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import sklearn, re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cluster_qualities(df, clustered_df, bugs_df, max_f, eps):\n",
    "    clusters_found = False\n",
    "    count = clustered_df['cluster_#'].max()\n",
    "    if math.isnan(count):\n",
    "        count = 0\n",
    "    for clu in set(df['cluster_dbscan']):\n",
    "        if clu >= 0:\n",
    "            temp_df = df[df['cluster_dbscan']==clu]\n",
    "            if eps <= 0.2:\n",
    "                decision_boundry = 0.7\n",
    "            else:\n",
    "                decision_boundry = 0.3\n",
    "            if temp_df['cluster_dbscan_silhouette'].mean() > decision_boundry:\n",
    "                count += 1\n",
    "                clusters_found= True\n",
    "                intracluster_ids = bugs_df[bugs_df['id'].isin(temp_df['id'])][['id', 'reporter_id', 'severity', 'priority']]\n",
    "                intracluster_ids['cluster'] = '{}_{}_{}'.format(max_f, eps, clu, count)\n",
    "                intracluster_ids['cluster_#'] = count\n",
    "                intracluster_ids['cluster_dbscan_silhouette_mean'] = temp_df['cluster_dbscan_silhouette'].mean()\n",
    "                clustered_df = pd.concat([clustered_df, intracluster_ids], axis=0)\n",
    "                print \"{} issues clustered\".format(len(clustered_df))\n",
    "    return clustered_df, clusters_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regex_square_brackets = re.compile(r'(\\[)|(\\])')\n",
    "bugs = pd.read_csv('../datasets/lexical_semantic_preprocessed_mantis_bugs_less_columns.csv')\n",
    "bug_notes = pd.read_csv('../datasets/lexical_semantic_preprocessed_mantis_bugnotes.csv')\n",
    "bug_notes['bug_note'] = bug_notes['bug_note'].str.replace(regex_square_brackets, '')\n",
    "bugs['additional_information'] = bugs['additional_information'].str.replace(regex_square_brackets, '')\n",
    "bugs['description'] = bugs['description'].str.replace(regex_square_brackets, '')\n",
    "bugs['summary'] = bugs['summary'].str.replace(regex_square_brackets, '')\n",
    "\n",
    "df_bug_note_table = bug_notes.groupby(['bug_id'])['bug_note'].apply(','.join).to_frame('bug_notes').reset_index()\n",
    "result = pd.merge(bugs, df_bug_note_table, how='left', left_on='id', right_on='bug_id')\n",
    "result.fillna('', inplace=True)\n",
    "result['textual_data'] = result['summary'] + ',' + result['description'] + ',' + result['additional_information'] + ',' + result['bug_notes']\n",
    "\n",
    "max_features = 50\n",
    "clustered = pd.DataFrame(columns=['id', 'reporter_id', 'severity', 'priority', 'cluster', 'cluster_#', 'cluster_dbscan_silhouette_mean'])\n",
    "eps = 0.03\n",
    "# decrease features by 500 in each iteration\n",
    "# for each selection of max_features try to find good clusters\n",
    "# apply recursion to increase eps by 0.01 at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_iterations = True\n",
    "while(eps_iterations):\n",
    "    max_features = 500\n",
    "    while(True):\n",
    "        bug_bugnotes_df = result[['id', 'textual_data']].copy()\n",
    "        bug_bugnotes_df = bug_bugnotes_df[~(bug_bugnotes_df['id'].isin(clustered['id']))]\n",
    "        vectorizer = TfidfVectorizer(min_df=2, max_df=0.9, max_features=max_features, stop_words='english')\n",
    "        X = vectorizer.fit_transform(bug_bugnotes_df['textual_data'])\n",
    "        print \"Iteration for the combination max_features: {} and eps: {} started ...\".format(max_features, eps)\n",
    "        dbscan = DBSCAN(eps=eps, metric='cosine').fit_predict(X)\n",
    "        bug_bugnotes_df['cluster_dbscan'] = dbscan\n",
    "        clusters_found = False\n",
    "        if len(set(bug_bugnotes_df['cluster_dbscan'])) > 1:\n",
    "            sample_silhouette_values = silhouette_samples(X, dbscan)\n",
    "            bug_bugnotes_df['cluster_dbscan_silhouette'] = sample_silhouette_values\n",
    "            ### Search for good clusters\n",
    "            clustered, clusters_found = check_cluster_qualities(bug_bugnotes_df, clustered, bugs, max_features, eps)\n",
    "            #if clusters_found:\n",
    "             #   max_features = 50\n",
    "              #  eps = 0.03\n",
    "        if not clusters_found:\n",
    "            if max_features == 100:\n",
    "                max_features = 500\n",
    "            elif max_features == 500:\n",
    "                max_features = 1000\n",
    "            elif max_features == 1000:\n",
    "                max_features += 5000\n",
    "            #elif max_features <= 10000:\n",
    "            #    max_features += 2000\n",
    "            #elif max_features <= 20000:\n",
    "            #    max_features += 5000\n",
    "            else:\n",
    "                break\n",
    "    eps += 0.03\n",
    "    if eps > 0.4:\n",
    "        eps_iterations = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered.to_csv('../datasets/clusters_for_outsourcing', index = False)\n",
    "sources = clustered.groupby(['cluster_#', 'cluster_dbscan_silhouette_mean'])['cluster_#'].count().to_frame('issues').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results when threshold for silhouette coeffiecient is set to 0.5, 0.7 and 0.3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
