{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# Developer: Ali Hashaam (ali.hashaam@initos.com) <br>\n",
    "\\# 18th December 2018 <br>\n",
    "\n",
    "\\# Â© 2019 initOS GmbH <br>\n",
    "\\# License MIT <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To detect multiword phrases in sentences within a dataset,\n",
    "we are going to use gensim's model.phrases class https://radimrehurek.com/gensim/models/phrases.html\n",
    "to do that first we make have to make an object of the model.phrases class and utilize the method add\\_vocab\n",
    "to add all the tokenized sentences in it! and we will store the object with in a pickle file so that when we\n",
    "want to detect multiwords in sentences within the dataset under consideration, we will read pickle file \n",
    "and pass our sentences through the model.phrases object (we just read from pickle file) and it will put \n",
    "\\_ be the multiwords, making new york as new\\_york (considering if new_york is present frequently inside the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, time, string\n",
    "import nltk\n",
    "import pickle\n",
    "from gensim.models import Phrases\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentence_in_vocab(row, textual_columns):\n",
    "    \"\"\"tokenize and remove stopwords and then add the sentence in model.phrase object\"\"\"\n",
    "    global bigram\n",
    "    for col in textual_columns:\n",
    "        sentence = row[col].translate(None, string.punctuation)\n",
    "        sentence = unicode(sentence, \"utf-8\")\n",
    "        tokenize = nltk.word_tokenize(sentence)\n",
    "        stopWords = set(nltk.corpus.stopwords.words('english'))\n",
    "        sentence = [x for x in tokenize if x not in stopWords]\n",
    "        if len(sentence):\n",
    "            bigram.add_vocab([sentence])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df, textual_columns):\n",
    "    df = df.apply(lambda x: add_sentence_in_vocab(x, textual_columns), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    file_directory = \"../datasets/\"\n",
    "    csv_files = ['mantis_bug_notes', 'mantis_bugs']\n",
    "    #csv_files = ['mantis_en_issues'] \n",
    "    gensim_directory = \"../datasets/gensim_multiphrases/\"\n",
    "    bigram = Phrases()\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(file_directory + csv_file +'.csv').head(5)\n",
    "        if 'mantis_bug_notes' in csv_file:\n",
    "            textual_cols = ['bug_note']\n",
    "            id_col = 'bugnote_id'\n",
    "        elif 'mantis_bugs' in csv_file:\n",
    "            textual_cols = ['summary', 'description', 'additional_information', 'steps_to_reproduce']\n",
    "            id_col = 'id'\n",
    "        df = process_data(df, textual_cols)\n",
    "    with open(\"{}{}.pkl\".format(gensim_directory, '_'.join(csv_files)), \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(bigram, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename|textual_columns (comma seperated)|id_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
