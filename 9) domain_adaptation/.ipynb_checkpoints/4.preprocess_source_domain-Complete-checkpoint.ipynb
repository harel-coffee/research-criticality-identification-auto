{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# Developer: Ali Hashaam (ali.hashaam@initos.com) <br>\n",
    "\\# 3rd March 2019 <br>\n",
    "\n",
    "\\# Â© 2019 initOS GmbH <br>\n",
    "\\# License MIT <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk, os\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from pprint import pprint\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import wordnet\n",
    "import itertools, string, re, unidecode, time, pickle\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from nltk import bigrams\n",
    "import operator\n",
    "import ast, icu\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.scikitTSVM import SKTSVM\n",
    "from frameworks.SelfLearning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex to remove urls and emails from text\n",
    "regex_doublequotes = re.compile(r'\\\"+')\n",
    "regex_square_brackets = re.compile(r'(\\[)|(\\])')\n",
    "regex_urls = re.compile('http\\S+')\n",
    "regex_emails = re.compile('\\S*@\\S*\\s?')\n",
    "regex_tab_newlines = re.compile(r'(\\n+)|(\\r+)|(\\t+)')\n",
    "remove_html_tags = re.compile(r'<[^>]+>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CharsSet = \"ascii\" # The Character set to be used as the default one when interpreting texts\n",
    "casefold = lambda u: unicode(icu.UnicodeString(u).foldCase()).encode(CharsSet, \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordnetPos(_treebank_tag):\n",
    "    \"\"\"\n",
    "    Translate the tree bank PoS tags to the WordNet's\n",
    "    \n",
    "    > Parameters:\n",
    "        _treebank_tag : str     | The tag to be translated\n",
    "    \n",
    "    > Returns:\n",
    "        The relevant WordNet PoS tag\n",
    "    https://stackoverflow.com/a/15590384/3429115\n",
    "    \"\"\"\n",
    "    if _treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif _treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif _treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif _treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return '' # CAUTION! It will remove all the words but the four above! implicit stopwords and punctioation removal somehow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeTaggedTerms(_tgdSentsMTX, _isFlattened = True, _oddTokensBehaviour = 3):\n",
    "    \"\"\"\n",
    "    Infer the lemmatized form of tokens with their PoS tags.\n",
    "    \n",
    "    > Parameters:\n",
    "        _tgdSentsMTX : collection of pairs: token - PoS tag | The Token/Tag collection from which we want to find lemmas\n",
    "        \n",
    "        _isFlattened : bool                                 | True if the collection is an array rather than a matrix\n",
    "        \n",
    "        _oddTokensBehaviour : bool                          | What to do when an odd non-English or non-lingual token is encountered,\n",
    "                                                            | if 1, such token is not lemmatized and included as-is,\n",
    "                                                            | if 2, such token is  to be in ASCII form, ignoring non-ascii chars,\n",
    "                                                            | if 3, such token is included after replacing the odd chars with '?'.\n",
    "                                                            | 0 (and otherwise), such token is discarded.\n",
    "                                                            \n",
    "    > Returns:\n",
    "        The lemmatized list of tokens according to the selected behaviour.\n",
    "    \n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizedSentsMTX = []\n",
    "    for pair in _tgdSentsMTX:\n",
    "        WordNetTag = getWordnetPos(str(pair[1]));\n",
    "        if (len(WordNetTag) > 0): # Ensure there is a mapping to WordNet categories, ignore punctuations, propositions, determinants, etc.\n",
    "            # Append the lemmatized token to the sentence list after decoding foreign letters\n",
    "            # If lemmatizing fails, often due to non-English characters, kepp the word as it is:\n",
    "            try:\n",
    "                lemma = lemmatizer.lemmatize(pair[0], WordNetTag)\n",
    "                lemmatizedSentsMTX.append(lemma)\n",
    "            except UnicodeDecodeError: # This code won't be entered at all, since the controlle will convert the unicode to the ASCII chars only\n",
    "                if (_oddTokensBehaviour == 1):\n",
    "                    lemmatizedSentsMTX.append(pair[0])\n",
    "                elif (_oddTokensBehaviour == 2):\n",
    "                    lemmatizedSentsMTX.append(lemmatizer.lemmatize(unicode(pair[0], errors=\"ignore\").encode(CharsSet,\"ignore\"), WordNetTag))\n",
    "                elif (_oddTokensBehaviour == 3):\n",
    "                    lemmatizedSentsMTX.append(lemmatizer.lemmatize(unicode(pair[0], errors=\"replace\").encode(CharsSet,\"replace\"), WordNetTag))\n",
    "                        \n",
    "    return lemmatizedSentsMTX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagPoS(_text):\n",
    "    \"\"\"\n",
    "    TagPoS will find the PoS tags for a review, which is of a multiple sentences, preserving the sentences' boundaries.    \n",
    "    \n",
    "    > Parameters:\n",
    "        _text : str                 | the text of the review in str format.\n",
    "\n",
    "    > Returns:\n",
    "    the returned object is a mere array of words' tags, but the sentences boundaries would have been \n",
    "    taken into consideration nevertheless.\n",
    "    \n",
    "        Additionally, _removePunt will be taken into consideration as well.\n",
    "    \n",
    "    > Comments:\n",
    "        As said, it is NOT recommended to remove the punctuation blindly. However, this ability is offered. A better way is to remove \n",
    "    the punctuation from the output depending on smart PoS tagging, where the tag is \".\" === a punctuation.\n",
    "    \n",
    "        Also, It is not recommended to remove stop words before this stage, the outputs will contain all part of speeches, and afterwards \n",
    "    we can remove everything but adjectives, nouns, verbs, and adverbs, a smarter way to get the essence of a sentence.\n",
    "    \n",
    "        The main function here is documented on: http://www.nltk.org/api/nltk.tag.html#nltk.tag.pos_tag_sents\n",
    "    \"\"\"\n",
    "    #if not(isinstance(_text, unicode)):\n",
    "        #_text = unicode(_text, errors=\"ignore\");        \n",
    "    listSentences = nltk.sent_tokenize(_text);\n",
    "    # But even sentences need to be an array of words, so we have to tokenise further, making each sentence array distinguishable by rows\n",
    "    # Convert the list of sentences to a list of list of words:\n",
    "    matrixSentences = [];\n",
    "    for sentence in listSentences:\n",
    "        if not (isinstance(sentence, str)):\n",
    "            sentence = sentence.encode(CharsSet,\"ignore\");            \n",
    "        sentence = sentence.translate(None, string.punctuation)\n",
    "        # Append non-empty sentences:\n",
    "        if (len(sentence) > 0):\n",
    "            # Back to unicode:\n",
    "            sentence = unicode(sentence, errors=\"ignore\")\n",
    "            # bigram[nltk.word_tokenize(sentence)] will place _ with in multiword phrases if present, any. \n",
    "            #matrixSentences.append(bigram[nltk.word_tokenize(sentence)])\n",
    "            matrixSentences.append(nltk.word_tokenize(sentence))\n",
    "    # Now let's try to PoS on the sentences of the text:\n",
    "    taggedTokens = nltk.pos_tag_sents(matrixSentences)\n",
    "    taggedTokens = list(itertools.chain.from_iterable(taggedTokens))\n",
    "    return taggedTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeText(_text, _oddCharsBehaviour = 1):\n",
    "    \"\"\"\n",
    "    The main controller to lemmatize a text and return the list of lemmatized tokens.\n",
    "    \n",
    "    This controller takes care of: replacing foreign accents with most likely letters, casefolding, \n",
    "    removing newlines and tabs, replacing \" with ', removing punctuation after expanding abbreviations \n",
    "    for the sake of a better effectiveness, stopping, tokenizing, generating the PoS tags, and \n",
    "    generating lemmas of the text depending on PoS tags.\n",
    "    lemmatizeTaggedTerms\n",
    "    :parameters:\n",
    "        _text : unicode             | The text to tokenize and lemmatize, in unicode, which will be converted to str \n",
    "                                    | after proper processing\n",
    "        \n",
    "        _enAbbrvDict : dictionary   | Holds the english abbreviations' shorthands and expansions, used when \n",
    "                                    | removing punctuation\n",
    "        \n",
    "        _oddCharsBehaviour : bool   | What to do when an odd non-English or non-lingual token is encountered,\n",
    "                                    | if 1, such token is not lemmatized and included as-is,\n",
    "                                    | if 2, such token is  to be in ASCII form, ignoring non-ascii chars,\n",
    "                                    | if 3, such token is included after replacing the odd chars with '?'.\n",
    "                                    | 0 (and otherwise), such token is discarded.\n",
    "    \n",
    "    :returns:\n",
    "        A list of lemmatized tokens which belong to the inputted _text and semantic orientation(SO) score\n",
    "    \"\"\"    \n",
    "    # First, strip the unicode of accents, replace with Ã with ss\n",
    "    try:\n",
    "        _text = unidecode.unidecode(_text)\n",
    "        taggedText = tagPoS(casefold(_text))\n",
    "    except:\n",
    "        taggedText = tagPoS(casefold(_text))\n",
    "    return lemmatizeTaggedTerms(taggedText, _oddTokensBehaviour = _oddCharsBehaviour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lemmatization(row, textual_columns):\n",
    "    \"\"\"\n",
    "    apply lematizeText Function on every row of dataset.\n",
    "    \n",
    "    :parameters:\n",
    "        row : pandas Dataframe row | row to be processed\n",
    "\n",
    "    :returns:\n",
    "        processed row\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if users_df.loc[row['bug_note_reporter_id'], 'roles'] in ['Customers', 'Customers Suppliers', 'initOS Suppliers', \n",
    "                                                                 'Partner']:\n",
    "            is_customer = True\n",
    "        else:\n",
    "            is_customer = False\n",
    "    except:\n",
    "        is_customer = False\n",
    "    for col in textual_columns:\n",
    "        \n",
    "        _text = re.sub(regex_tab_newlines, \" \", row[col])\n",
    "        _text = re.sub(regex_doublequotes, \" \", _text)\n",
    "        _text = re.sub(regex_square_brackets, \" \", _text)\n",
    "        _text = re.sub(regex_urls, \" \", _text)\n",
    "        _text = re.sub(regex_emails, \" \", _text)\n",
    "        _text = re.sub(remove_html_tags, \" \", _text)\n",
    "        _text = unicode(_text, \"utf-8\")\n",
    "        row[col] = lemmatizeText(_text)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(df, textual_columns):\n",
    "    \"\"\"\n",
    "        remove the null values and then apply apply_lemmatization.\n",
    "\n",
    "        :parameters:\n",
    "            df : pandas Dataframe Object | Dataframe to be processed\n",
    "    \n",
    "        :returns:\n",
    "            Processed Dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    df = df.apply(lambda row: apply_lemmatization(row, textual_columns),axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "erp_nxt = pd.read_csv('../datasets/github_projects/erpnext_issues_relevant.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erp_nxt['text'] = erp_nxt['title'].fillna('') + ',' + erp_nxt['comments'].fillna('')\n",
    "erp_nxt.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pre_process_data(erp_nxt, ['text'])\n",
    "df['text'] = df['text'].apply(', '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[df['type'].str.contains('bug'), 'type'] = 0\n",
    "df.loc[df['type']!=0, 'type'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    734\n",
       "0    430\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('github_preprocessed_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs = pd.read_csv('../datasets/lexical_semantic_preprocessed_mantis_bugs_less_columns_with_class_expansion.csv')\n",
    "bug_notes = pd.read_csv('../datasets/lexical_semantic_preprocessed_mantis_bugnotes.csv')\n",
    "bug_notes['bug_note'] = bug_notes['bug_note'].str.replace(regex_square_brackets, '')\n",
    "bugs['additional_information'] = bugs['additional_information'].str.replace(regex_square_brackets, '')\n",
    "bugs['description'] = bugs['description'].str.replace(regex_square_brackets, '')\n",
    "bugs['summary'] = bugs['summary'].str.replace(regex_square_brackets, '')\n",
    "df_bug_note_table = bug_notes.groupby(['bug_id'])['bug_note'].apply(','.join).to_frame('bug_notes').reset_index()\n",
    "result = pd.merge(bugs, df_bug_note_table, how='left', left_on='id', right_on='bug_id')\n",
    "result['textual_data'] = result['summary'].fillna('') + ',' + result['description'].fillna('') + ',' + result['additional_information'].fillna('') + ',' + result['bug_notes'].fillna('')\n",
    "result['textual_data'] = result['textual_data'].str.replace(\" \", \"\")\n",
    "result.sort_values(by=['class'], inplace=True)\n",
    "result.reset_index(drop=True, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DA_insight = result[~result['class'].isnull()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DA_insight = DA_insight[DA_insight['severity'].isin([10, 40, 70, 80])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class         severity\n",
       "critical      10           19\n",
       "              40            2\n",
       "              70            9\n",
       "              80           39\n",
       "non-critical  10          737\n",
       "              40           42\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DA_insight.groupby(['class', 'severity']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs = result[result['severity'].isin([40, 70, 80])].index\n",
    "non_bugs = result[result['severity'].isin([10])].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"bug_or_not\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.loc[bugs,'bug_or_not'] = 'bugs'\n",
    "result.loc[non_bugs,'bug_or_not'] = 'non_bugs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DA_insight = result[~result['bug_or_not'].isnull()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>bug_or_not</th>\n",
       "      <th>bugs</th>\n",
       "      <th>non_bugs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>critical</th>\n",
       "      <td>50</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>non-critical</th>\n",
       "      <td>42</td>\n",
       "      <td>737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "bug_or_not    bugs  non_bugs\n",
       "class                       \n",
       "critical        50        19\n",
       "non-critical    42       737"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DA_insight.groupby(['class', 'bug_or_not']).size().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "DA_insight.groupby(['class', 'bug_or_not']).size().unstack().to_csv('bug_vs_critical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    756\n",
       "0.0     92\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.loc[result['bug_or_not']=='bugs', 'type'] = 0\n",
    "result.loc[result['bug_or_not']=='non_bugs', 'type'] = 1\n",
    "result['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('mantis_data_for_domain_adaptation.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
