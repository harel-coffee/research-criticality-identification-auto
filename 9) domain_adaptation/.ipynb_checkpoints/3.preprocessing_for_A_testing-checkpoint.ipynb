{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# Developer: Ali Hashaam (ali.hashaam@initos.com) <br>\n",
    "\\# 2nd March 2019 <br>\n",
    "\n",
    "\\# Â© 2019 initOS GmbH <br>\n",
    "\\# License MIT <br>\n",
    "\n",
    "The code is responsible for preprocessing data for PAD testing.<br>\n",
    "https://github.com/rpryzant/proxy-a-distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re, icu\n",
    "from string import punctuation\n",
    "import os.path\n",
    "import random, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the main output folder, in which the text files are placed, and checks are made to avoid duplicated work:\n",
    "directory = \"../datasets/\"\n",
    "datasets_dir = \"../datasets\"\n",
    "regex_doublequotes = re.compile(r'\\\"+')\n",
    "regex_square_brackets = re.compile(r'(\\[)|(\\])')\n",
    "regex_urls = re.compile('http\\S+')\n",
    "regex_emails = re.compile('\\S*@\\S*\\s?')\n",
    "regex_tab_newlines = re.compile(r'(\\n+)|(\\r+)|(\\t+)')\n",
    "remove_html_tags = re.compile(r'<[^>]+>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def establish_logger():\n",
    "    logger = logging.getLogger(\"PAD_TESTING\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    fh = logging.FileHandler('{}/logs/pad_testing.log'.format(datasets_dir))\n",
    "    fh.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The py2casefold in not tested enough, slow, and I couldn't install it in anaconda. This lambda function is a replacement.\n",
    "The function returns a string, not a unicode, because we will use the .translate fast function to remove punct. of strings.\n",
    "Source: https://stackoverflow.com/a/32838944/3429115\n",
    "\"\"\"\n",
    "CharsSet = \"ascii\" # The Character set to be used as the default one when interpreting texts\n",
    "casefold = lambda u: unicode(icu.UnicodeString(u).foldCase()).encode(CharsSet,\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_sample_fast(iterable, samplesize):\n",
    "    \"\"\"\n",
    "    Fast memory-efficient sampling method for pretty large iterables.\n",
    "    Adopted from: https://stackoverflow.com/a/12583436/3429115\n",
    "    \n",
    "    > Parameters:\n",
    "        * iterable: iterable object | The collection we want to sample from\n",
    "        \n",
    "        * samplesize: int           | How many samples to generate\n",
    "        \n",
    "    > Returns:\n",
    "        List of samples, hoewver, since sampling is made without replacement (most probably), they aren't IID; but\n",
    "        it's not that problem to our case I guess, especially that we do not want to sample the same item twice...\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    iterator = iter(iterable)\n",
    "    # Fill in the first samplesize elements:\n",
    "    try:\n",
    "        for _ in xrange(samplesize):\n",
    "            results.append(iterator.next())\n",
    "    except StopIteratlion:\n",
    "        raise ValueError(\"Sample larger than population.\")\n",
    "    random.shuffle(results)  # Randomize their positions\n",
    "    for i, v in enumerate(iterator, samplesize):\n",
    "        r = random.randint(0, i)\n",
    "        if r < samplesize:\n",
    "            results[r] = v  # at a decreasing rate, replace random items\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elicitDomainSentences(df_Reviews, _Amount = 0):\n",
    "    \"\"\"\n",
    "    Convert a domain csv resource to a list of sentences, casefolded and stripped from common nuisances. If a sample\n",
    "    is required, then random sampling is applied after generating the full population.\n",
    "    \n",
    "    > Parameters:\n",
    "        * df_Reviews: Pandas Dataframe - dataset whose textual data needs preprocessing\n",
    "    \n",
    "    > Returns:\n",
    "        The list of sentences of the domain in a list. Sentences that contain < 3 words aren't considered.\n",
    "    \"\"\"                             \n",
    "    # Drop rows if text is NaN\n",
    "    df_Reviews.dropna(inplace=True)    \n",
    "    \n",
    "    casefolded = np.array(df_Reviews[\"text\"].apply(casefold))\n",
    "\n",
    "    SentencesReviewsDomain = []\n",
    "    counter = 0\n",
    "    total = len(casefolded)\n",
    "    \n",
    "    for review in casefolded:        \n",
    "        if (counter % 1000 == 0):\n",
    "            logger.info(\"{}% of data processed..\".format(round(100.0 * counter / total,2)))\n",
    "        # Replace newlines and double quotes  \n",
    "        review = re.sub(regex_tab_newlines, \" \", review)\n",
    "        review = re.sub(regex_doublequotes, \"'\", review)\n",
    "        review = re.sub(regex_square_brackets, \"\", review)\n",
    "        review = re.sub(regex_urls, \" \", review)\n",
    "        review = re.sub(regex_emails, \" \", review)\n",
    "        review = re.sub(remove_html_tags, \" \", review)\n",
    "        #Split sentences:\n",
    "        currentSentences = sent_tokenize(review)\n",
    "        #remove punctuation:\n",
    "        nonPunctuatedSentences = []\n",
    "        for s in currentSentences:\n",
    "            if (len(s.split()) >= 3): # Only consider the sentence if it has at least 3 words.\n",
    "                nonPunctuatedSentences.append(s.translate(None, punctuation)) # Remove punctuation\n",
    "                \n",
    "        if (len(nonPunctuatedSentences) > 0):\n",
    "            SentencesReviewsDomain.extend(nonPunctuatedSentences)\n",
    "        counter += 1\n",
    "    logger.info(\"100% of data processed.\")\n",
    "    print len(df_Reviews)\n",
    "    print len(SentencesReviewsDomain)\n",
    "    if (type(_Amount) == int and _Amount > 0 and _Amount < len(SentencesReviewsDomain)):\n",
    "        \"\"\" Apply the random sampling if proper and required:\n",
    "         The population has been generated, and now we want to select _Amount i.i.d samples;\n",
    "         To choose i.i.d samples, sampling WITH replacement must be carried out.\n",
    "         Choosing a specific _Amount from all domains guarantees the balance of the U dataset for PAD,\n",
    "         And enhances the efficiency of the PAD SVM of course: two birds with one stone. \"\"\"\n",
    "        # This np.random.choice causes memory problems with large domains, such as restaurants |-_-|..\n",
    "        # Additionally, it will sometimes sample the same item more than once!\n",
    "        #return np.random.choice(SentencesReviewsDomain, _Amount).tolist();\n",
    "        # We will use a more efficient way of sampling without sampling the same item twice:\n",
    "        return iter_sample_fast(SentencesReviewsDomain, _Amount)\n",
    "    else:\n",
    "        # No sampling, return the mere list: \n",
    "        return SentencesReviewsDomain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCommonVocabularyFromSentences(_domain1, _domain2):\n",
    "    \"\"\"\n",
    "    Find the intersection of vocabulary between the two domains, expressed as lists of sentences, so it is best\n",
    "    called after `elicitDomainSentences` routine.\n",
    "    \n",
    "    > Parameters:\n",
    "        * _domain1: list      | List of sentences belonging to the first domain;\n",
    "        \n",
    "        * _domain2: list      | List of sentences belonging to the second domain.\n",
    "    \n",
    "    > Returns:\n",
    "        The set of common vocabulary between the two domains.\n",
    "    \"\"\"\n",
    "    if ((len(_domain1)==0) or (len (_domain2)==0)):\n",
    "        logger.info(\"ATTENTION: One or more empty domain(s) passed to findCommonVocabulary.\")\n",
    "        return None\n",
    "    \n",
    "    domain1Vocab = []\n",
    "    for s in _domain1:     \n",
    "        domain1Vocab.extend(s.split())\n",
    "        \n",
    "    domain2Vocab = []\n",
    "    for s in _domain2:\n",
    "        domain2Vocab.extend(s.split())\n",
    "    \n",
    "    return set.intersection(set(domain1Vocab), set(domain2Vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveList2txt(_List, _filePath, IsVocabulary = False):\n",
    "    \"\"\"\n",
    "    Saves a list to a text file, each entry on a new line.\n",
    "    https://stackoverflow.com/a/13434105/3429115\n",
    "    \n",
    "    > Parameters:\n",
    "      * _List: list             | The list to be saved\n",
    "      \n",
    "      * _filePath: string       | The full path, including file name and extension\n",
    "      \n",
    "      * IsVocabulary: boolean   | True if we are saving the vocabulary, so that we prepend the special values\n",
    "    \n",
    "    > Returns:\n",
    "        0 if the execution goes well, -1 if the file already exists.\n",
    "    \"\"\"\n",
    "    if (os.path.isfile(_filePath)):\n",
    "        logger.info(\"ATTENTION: text file already exists, exiting.\")\n",
    "        return -1\n",
    "    \n",
    "    outfile = open(_filePath, \"w\")\n",
    "    if (IsVocabulary):\n",
    "        outfile.write(\"<unk>\\n<s>\\n</s>\\n\")\n",
    "    print >> outfile, \"\\n\".join(s.strip() for s in _List)\n",
    "    outfile.close()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSentencesFromtxt(_Filepath):\n",
    "    \"\"\"\n",
    "    Load sentences list for a specific domain from disk.\n",
    "    \n",
    "    > Parameters:\n",
    "        * _Filepath: the path to the text file, in which each sentence is expected to be on one line. No checks are done\n",
    "        to ensure the file exists, so be sure it does, or the function will vomit an exception, I guess.\n",
    "    \"\"\"\n",
    "    dataSentences = [];\n",
    "    for line in open(_Filepath):\n",
    "        dataSentences.append(line.strip());\n",
    "    \n",
    "    return dataSentences;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentencesForDomain(directory, _Filename, df, _NumberOfSentences):\n",
    "    \"\"\"\n",
    "    Processes domains into sentences \n",
    "    \n",
    "    > Parameters:\n",
    "        * _Filename : string       | First domain's name in the common dataset directory -without the extension\n",
    "        \n",
    "        * df : Pandas Dataframe       | Second domain's name in the common dataset directory -without the extension\n",
    "    \n",
    "    > Returns:\n",
    "        * sentences: list        | list of all the sentences of domains\n",
    "    \"\"\"\n",
    "    domainAlreadyDone = os.path.isfile(\"{}/{}_sentences.txt\".format(directory, _Filename))    \n",
    "    if (not domainAlreadyDone):\n",
    "        sentences = elicitDomainSentences(df, _NumberOfSentences)\n",
    "        logger.info(\"{} data read and processed..\".format(_Filename))\n",
    "        saveList2txt(sentences, \"{}/{}_sentences.txt\".format(directory, _Filename))\n",
    "        logger.info(\"{} data saved ({:d} sentences).\".format(_Filename, len(sentences)))\n",
    "    else:\n",
    "        logger.info(\"ATTENTION: Domain {} was found to be already processed.\".format(_Filename))\n",
    "        # Load sentences\n",
    "        logger.info(\"INFO: Domain's sentences loaded as a preface to build the common vocabulary.\")\n",
    "        sentences = loadSentencesFromtxt(\"{}/{}_sentences.txt\".format(directory, _Filename))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessDomains(_Filename1, _Filename2, df_A, df_B, _NumberOfSentences):\n",
    "    \"\"\"\n",
    "    The main controller; Processes domains into sentences, finds their common vocabulary, and saves the results. Work is\n",
    "    carried out in a structured way to avoid redundant tasks and unintentional outputs overwriting.\n",
    "    \n",
    "    > Parameters:\n",
    "        * _Filename1 : string       | First domain's name in the common dataset directory -without the extension\n",
    "        \n",
    "        * _Filename2 : string       | Second domain's name in the common dataset directory -without the extension\n",
    "        \n",
    "    > Returns:\n",
    "        None. Everything is written to disk in `directory` path.\n",
    "    \"\"\"\n",
    "    sentencesD1 = []\n",
    "    sentencesD2 = []    \n",
    "    logger.info(\"Starting main routine..\")\n",
    "    # Is the vocabulary already there?\n",
    "    IsVocabAlreadyBuilt = os.path.isfile(\"{}/Vocab_{}_{}.txt\".format(directory, _Filename2, _Filename1))\n",
    "    if (not IsVocabAlreadyBuilt):\n",
    "        sentencesD1 = getSentencesForDomain(directory, _Filename1, df_A, _NumberOfSentences)\n",
    "        sentencesD2 = getSentencesForDomain(directory, _Filename2, df_B, _NumberOfSentences)\n",
    "        setCommonVocab = findCommonVocabularyFromSentences(sentencesD1, sentencesD2)\n",
    "        logger.info(\"Common vocabulary for {} and {} processed..\".format(_Filename1, _Filename2))\n",
    "        saveList2txt(setCommonVocab, \"{}/Vocab_{}_{}.txt\".format(directory, _Filename2, _Filename1), IsVocabulary=True)\n",
    "        logger.info(\"Common vocabulary saved ({:d} terms).\".format(len(setCommonVocab)))\n",
    "        logger.info(\"All Done.\")\n",
    "    else:\n",
    "        logger.info(\"ATTENTION: Vocabulary was already found on disk, so no need to rebuild it again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_textual_data_in_one_column(filename, _NumberOfSentences = 0):\n",
    "    \"\"\"\n",
    "    merge textual data into one column\n",
    "    \n",
    "    > Parameters:\n",
    "    * plateform: str- Name of plateform whose textual data is to be dealt with\n",
    "    * _NumberOfSentences : int  | The number of sentences to include from both of the domains. 0 means all sentences,\n",
    "                                | and > 0 means random sampling will be applied to pick this number of sentences out\n",
    "                                | of the domain's population of sentences.\n",
    "    > Returns:\n",
    "    df: Pandas Dataframe- dataframe with two columns id and text, with text containing all the textual\n",
    "        data belonging to particular id\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('{}/jira_projects/{}.csv'.format(datasets_dir, filename))\n",
    "    if 'mantis' in filename:\n",
    "        df = df[df[\"translation_status\"]=='done']\n",
    "    df = df.fillna('')\n",
    "    \"\"\" Apply the random sampling if proper and required:\n",
    "         The population has been generated, and now we want to select _Amount i.i.d samples\n",
    "         To choose i.i.d samples, sampling WITH replacement must be carried out.\n",
    "         Choosing a specific _Amount from all domains guarantees the balance of the U dataset for PAD,\n",
    "         And enhances the efficiency of the PAD SVM of course: two birds with one stone. \"\"\"\n",
    "    if _NumberOfSentences > 0:\n",
    "        df = df.sample(n=_NumberOfSentences, replace=False, weights=None)\n",
    "    \n",
    "    if 'mantis' in plateform:\n",
    "        df['text'] = df['description'] + '. ' +  df['summary'] + '. ' +  df['bug_notes']  + '. '  +  df['steps_to_reproduce'] + '. ' +  df['additional_information']\n",
    "    else:\n",
    "        df['text'] = df['description'] + '. ' +  df['summary'] + '. ' +  df['comments']\n",
    "    return df[['id', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_NumberOfSentences = 0\n",
    "bug_notes = pd.read_csv('../datasets/mantis_bug_notes_en.csv')\n",
    "bugs = pd.read_csv('../datasets/mantis_bugs_en.csv')\n",
    "df_bug_note_table = bug_notes.groupby(['bug_id'])['bug_note'].apply(','.join).to_frame('bug_notes').reset_index()\n",
    "result = pd.merge(bugs, df_bug_note_table, how='left', left_on='id', right_on='bug_id')\n",
    "result['text'] = result['summary'].fillna('') + ',' + result['description'].fillna('') + ',' + result['additional_information'].fillna('') + ',' + result['bug_notes'].fillna('')\n",
    "if _NumberOfSentences > 0:\n",
    "    result = result.sample(n=_NumberOfSentences, replace=False, weights=None)\n",
    "mantis_df = result[['id', 'text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for erp_next\n",
    "erp_nxt = pd.read_csv('../datasets/github_projects/erpnext_issues.csv')\n",
    "erp_nxt['type'] = erp_nxt['type'].str.lower()\n",
    "erp_nxt = erp_nxt[(erp_nxt['type'].str.contains('feature')) | (erp_nxt['type'].str.contains('bug')) | \n",
    "       (erp_nxt['type'].str.contains('manufacturing')) | (erp_nxt['type'].str.contains('Enhancement'))]\n",
    "erp_nxt.rename(columns={\"textual_data\": \"text\"}, inplace=True)\n",
    "erp_nxt_df = erp_nxt[['id', 'text']].copy()\n",
    "logger = establish_logger()\n",
    "ProcessDomains('mantis_en_issues', 'erp_next_issues', mantis_df, erp_nxt_df, 10000)\n",
    "erp_nxt.to_csv('../datasets/github_projects/erpnext_issues_relevant.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
